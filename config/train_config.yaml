model:
  model: "unsloth/Llama-3.2-1B"
  tokenizer: "unsloth/Llama-3.2-1B"
  use_cache: false
  max_length: 128

dataset:
  path: data/cleaned_llama_extracted_pages.jsonl
  split: "train"

train:
  output_dir: output
  run_name: "llama3.2_run" 
  num_train_epochs: 3
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  max_grad_norm: 1.0 
  evaluation_strategy: "steps"
  save_strategy: "steps"
  save_steps: 2000
  logging_steps: 100
  fp16: true
  deepspeed: "config/ds_config.json"
